---
type: page
title: The Big O Notation
tag: Algorithms
author: Namish
---

## Time and Space Complexity

![big-0](/bigo/big-o.png)

### Need For Big O and Time Complexity

Suppose these two codes written in python. Their task is to find the letter **i** in the word inputted by the user.

```py
word = input()
if i in word:
    print("Found i")
```

The second code

```py
word = input()
for letter in word:
    if letter == 'i':
        print("Found i")
```

Now if we were to run the first code on a piece of junk, and second one on a supercomputer, the supercomputer might execute it faster, but in the terms of Big O, the first code is faster. This is because the first code has a time complexity of **O(1)** and the second code has a time complexity of **O(n)**.

<br/>
So, Time Complexity is a way to represent how much time an algorithm takes to run, in terms of the input size. How do we see it mathematically? Well we can plot a graph of the time taken by the algorithm to run, against the input size. This graph is called the Time Complexity Graph. So let us take some example code.

```c
int main() {
    for (int i = 0; i < n; i++) {
        printf("Hello, World!");
    }
}
```

Here we can see that **n** and time are directly proportional. Therefore the graph would look something like this. Theta in the graph is called the rate of increase of time taken by the algorithm. And this rate is referred to as the Time Complexity of the algorithm.

![graph](/bigo/simple-graph.png)

Similarly space complexity is a way to represent how much space an algorithm takes to run, in terms of the input size. It is also represented in terms of the input size.

### Big O Notation

Big O Notation is a way to represent the time complexity of an algorithm. It is a mathematical notation that describes the limiting behavior of a function when the argument tends **towards a particular value** or **infinity**. It is used to **describe the upper bound** of the time complexity of an algorithm. It's like saying, "The time complexity of this algorithm **will never be more than this.**" We write as O(x), where x is the term of time complexity with the highest degree (without the coeffecient) of the algorithm. You just need to remember 3 rules while calculating the Big O of an algorithm.

1. Always consider the worst case scenario.
2. Ignore the constants.
3. Ignore the lower terms.

### Analyzing Algorithms

Let's take an example of a simple algorithm to swap two variables.

```c
void swap(int a, int b) {
    int temp = a;
    a = b;
    b = temp;
}
```

We will assume that each statement in the code takes 1 unit of time to execute. So the time function would be **T(n) = 1 + 1 + 1 = 3**. For the space complexity, we have **a**, **b**, and the **temp** variable, therefore the space function would be **S(n) = 1 + 1 + 1 = 3**. In terms of Big O, the time complexity would be **O(1)** and the space complexity would also be **O(1)**.

### Frequncy Count Method

This method is used to calculate the time complexity of an algorithm. It is done by counting the number of times a statement is executed. Let's take an example of a simple algorithm to find the sum of the first **n** natural numbers.

```c
int main() {
  sum = 0;
  n = 10;
  for (int i = 1; i <= n; i++) {
    sum += i;
  }
}
```

Here the for loop is actually made of three smaller statements. The initialization of **i**, the condition, and then finally, the increment. The initialization would only be executed once. The condition **i \<= n** would be checked for a **n+1** times (because it is \<=) and then finally the increment will happen **n** times.

Therefore, the time complexity of this algorithm would be **T(n) = 1 + 1 + 1 + n + n = 2n + 3** (the beginning 2 ones are for initializing sum and n).
In terms of Big O, the time complexity would be **O(n)**.

For the space complexity, we have **sum**, **n**, and **i**, therefore the space function would be **S(n) = 1 + 1 + 1 = 3**.
In terms of Big O, the space complexity would be **O(1)**.

<br/>
Let us take another example of adding two matrices, where the size is given as **n x n**.

```c
void add(int *A, int *B, int *C, int n) {
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < n; j++) {
            C[i * n + j] = A[i * n + j] + B[i * n + j];
        }
    }
}
```

Now we know that the first for loop would run for **n** times, the second nested for loop would run for **n * (n + 1)** times and each line in the second nested loop would run for **n * n** times. Therefore the time complexity turns out to  be **T(n) = n^2 + n + n^2 = 2n^2 + n**

For the space complexity, we have A, B, C and i,j,n.

A, B and C are 2 dimensional arrays, therefore they would take **n * n** space. And i, j, n would take **1** space. Therefore the space complexity would be **S(n) = 3n^2 + 3**.
In terms of Big O, the time complexity would be **O(n^2)** and the space complexity would also be **O(n^2)**.

### Some Rapid Fire Examples

```c
void fun(int n) {
    for (int i = 0; i < n; i+=2) {
        printf("%d", i);
    }
}
```

So what if instead of 1 step increment, it was two steps like shown above? Well the for loop would run for `n/2` times. Therefore the time complexity would be **O(n/2)**. But we ignore the constants, therefore the time complexity would be **O(n)**.

The space complexity would be **S(n) = 2**, so the space complexity would be **O(1)**.

```c
void fun(int n) {
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < i; j++) {
            printf("%d", i + j);
        }
    }
}
```

The above code is just a classic double nested loop but instead of j running for n times, it runs for i times. If it confuses you let me illustrate it.

![calculation](/bigo/calc.png)

So this code as well will have the time complexity of **O(n^2)**. Space complexity would be **O(1)**.

```c
for (int i = 1; i < n; i*=2) {
    printf("%d", i);
}
```

Calculating the time complexity of this code is kind of interesting. So after the code has run, the value of **i** would be greater than **n**. And we also know that the value of I would be a power of 2, let it be **2^k** So after the loop, we can say that
<br/>
<div align="center">
$i >= n$

$i = 2^k$

$2^k >= n$

$k >= log_2(n)$
</div>
<br/>

The time complexity would be **O(log(n))**. The space complexity would be **O(1)**. Let us solve another problem this way

```c
for (int i = 1; i*i < n; i++) {
    printf("%d", i);
}
```

After the loop has run, the value of **i^2** will be greater than **n**

<br/>
<div align="center">
$i^2 >= n$

$i >= \sqrt{n}$
</div>
<br/>

Therefore the time complexity would be **O(sqrt(n))**. The space complexity would be **O(1)**.

```c
for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j*=2) {
        printf("Hello");
    }
}
```

The inner loop will have a time complexity of **O(log(n))** and the outer loop will have a time complexity of **O(n)**. The time complexity of the code now becomes **O(n * log(n))**. The space complexity would be **O(1)**.


### Analysis of IF and WHILE

Time analysis of while loops are similar to that of for loops. Let us examine a while loop

```c
int i = 0;
while (i < n) {
    printf("%d", i);
    i++;
}
```

Like the for loop, the condition check would run for **n+1** times, all the lines in the for loop would run for **n** times, and there is an initialization statement of **i** which will take 1 unit of time. Therefore the time complexity would be **T(n) = 1 + n + 1 + n + n = 3n + 2** or **O(n)**. The space complexity would be **O(1)**.

<br/>

If conditions only divide the execution of code into two parts, which can give varying time complexities for different inputs.

```c
if (n % 2 == 0) {
    printf("Even");
} else {
    for (int i = 0; i < n; i++) {
        printf("%d", i);
    }
}
```

So in this case, if the input is even, the time complexity would be **O(1)**, but if the input is odd, the time complexity would be **O(n)**.  So **O(1)** is the best case time complexity and **O(n)** is the worst case time complexity.

### Common Time Complexities

During solving questions, here are some common time complexities that you will encounter. Here are them in order of increasing time complexity.
<br/>

<div align="center">
$ O(1) < O(log(n)) < O(\sqrt{n}) < O(n) < O(nlog(n)) < O(n^2) < O(2^n) < O(n!) $
</div>

<br/>

Here is a rough graph of the time complexities

![graph](/bigo/graph.png)
